{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blistt/bp-recommender/blob/Romasa/BP_Recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yhvxGLihuQYJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "path = 'data/'\n",
        "baseline = pd.read_csv(path + 'baseline.csv')\n",
        "augmented_k = pd.read_csv(path + 'augmented_k.csv')\n",
        "augmented_inter = pd.read_csv(path + 'augmented_inter.csv')\n",
        "augmented_intra = pd.read_csv(path + 'augmented_intra.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bp_predictor import BloodPresurePredictor\n",
        "from utils import log_exp, data_split, historical_BP\n",
        "\n",
        "def experiment(dataset, model, ntrees, N, key, target, log_path='', bootstrap=False, \n",
        "               bootstrap_size=0.8, aug='None', historical=True):\n",
        "    # Add historical blood pressure to the dataset if specified\n",
        "    if historical:\n",
        "        dataset = historical_BP(dataset, 3)\n",
        "\n",
        "    # Split dataset into train and test sets of features and labels\n",
        "    (x_train, y_train), (x_test, y_test) = data_split(dataset, y_columns=target, key_cols=key)\n",
        "    x_train = x_train.drop(key, axis=1)\n",
        "    x_test = x_test.drop(key, axis=1)\n",
        "\n",
        "    # First run with all features (either bootstrapped or not)\n",
        "    bp_predictor = BloodPresurePredictor(model, ntrees)\n",
        "    bp_predictor.fit(x_train, y_train, bootstrap, bootstrap_size)\n",
        "    # Evaluate the model\n",
        "    bp_predictor.evaluate(x_test, y_test)\n",
        "    \n",
        "    # log results\n",
        "    log_exp(log_path, bp_predictor, aug=aug, N=N, second_run=False, bootstrap=bootstrap, test_size=x_test.shape)     \n",
        "    \n",
        "    # Second run with top N features\n",
        "    top_n = list(bp_predictor.feature_importances.keys())[:N]        # get top N features from dict \n",
        "    x_train = x_train[top_n]                                         # select top N features\n",
        "    bp_predictor.fit(x_train[top_n], y_train)                        # predict with top N features\n",
        "    bp_predictor.evaluate(x_test[top_n], y_test)                     # evaluate with top N features\n",
        "    # log results\n",
        "    log_exp(log_path, bp_predictor, aug=aug, N=N, second_run=True, bootstrap=bootstrap, test_size=x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bp_predictor import BloodPresurePredictor\n",
        "from utils import log_exp, get_unique_healthCodes, average_dicts, strat_data_split\n",
        "from collections import defaultdict\n",
        "\n",
        "def personalized_experiment(dataset, model, ntrees, N, key, target, log_path='', bootstrap=False, \n",
        "               bootstrap_size=0.8, aug='None', second_run=False, save_path=None, historical=True):\n",
        "    # Add historical blood pressure to the dataset if specified\n",
        "    if historical:\n",
        "        dataset = historical_BP(dataset, 3)\n",
        "    \n",
        "    # Split dataset into train and test sets of features and labels\n",
        "    (x_train, y_train), (x_test, y_test) = strat_data_split(dataset, y_columns=target, key_cols=key)\n",
        "    x_train_keys = x_train[key]\n",
        "    x_test_keys = x_test[key]\n",
        "    x_train = x_train.drop(key, axis=1)\n",
        "    x_test = x_test.drop(key, axis=1)\n",
        "\n",
        "\n",
        "    # First run with all features (either bootstrapped or not)\n",
        "    bp_predictor = BloodPresurePredictor(model, ntrees)\n",
        "    bp_predictor.fit(x_train, y_train, bootstrap, bootstrap_size)\n",
        "\n",
        "    # Get all unique healthCodes\n",
        "    all_users = get_unique_healthCodes(dataset)\n",
        "\n",
        "    # Initialize lists to store metrics results\n",
        "    mae = defaultdict(list)\n",
        "    mse = defaultdict(list)\n",
        "    temp_feature_importances = []\n",
        "    # Personalize the model for each user\n",
        "    for user in all_users:\n",
        "        tr_mask = x_train_keys.iloc[:, 0] == user\n",
        "        test_mask = x_test_keys.iloc[:, 0] == user\n",
        "        x_train_user, y_train_user = x_train[tr_mask], y_train[tr_mask]\n",
        "\n",
        "        x_test_user, y_test_user = x_test[test_mask], y_test[test_mask]\n",
        "\n",
        "        # Skips if there are no samples for the user\n",
        "        if x_train_user.shape[0] < 1 or x_test_user.shape[0] < 1:\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            bp_predictor.fine_tune(x_train_user, y_train_user)                   # Fit the personalized model\n",
        "            bp_predictor.evaluate(x_test_user, y_test_user, fine_tuned=True)     # Evaluate the personalized model\n",
        "            # Performs second run with top N features if specified\n",
        "            if second_run:\n",
        "                top_n = list(bp_predictor.feature_importances.keys())[:N]\n",
        "                bp_predictor.fine_tune(x_train_user[top_n], y_train_user)\n",
        "                bp_predictor.evaluate(x_test_user[top_n], y_test_user, fine_tuned=True)\n",
        "            for bp_type in target:\n",
        "                mae[bp_type].append(bp_predictor.mae[bp_type])\n",
        "                mse[bp_type].append(bp_predictor.mse[bp_type])\n",
        "            temp_feature_importances.append(bp_predictor.feature_importances)\n",
        "\n",
        "            # Saves the model and the feature importances for the user\n",
        "            if save_path:\n",
        "                for bp_type in target:\n",
        "                    bp_predictor.ftmodel[bp_type].save_model(f'{save_path}/model_states/{user}_{bp_type}.json')\n",
        "                # Save dict of feature importances\n",
        "                with open(f'{save_path}/feature_importances/{user}.json', 'w') as f:\n",
        "                    f.write(str(bp_predictor.feature_importances))\n",
        "\n",
        "    if len(mae) == 0:\n",
        "        print('No testing samples')\n",
        "        return\n",
        "    \n",
        "    # Average metrics for all users\n",
        "    for bp_type in target:\n",
        "        bp_predictor.mae[bp_type] = sum(mae[bp_type]) / len(mae[bp_type])\n",
        "        bp_predictor.mse[bp_type] = sum(mse[bp_type]) / len(mse[bp_type])\n",
        "    bp_predictor.feature_importances = average_dicts(temp_feature_importances)\n",
        "\n",
        "    # log results\n",
        "    log_exp(log_path, bp_predictor, aug=aug, N=N, second_run=second_run, bootstrap=bootstrap, \n",
        "            test_size=x_test.shape, personalized=True)\n",
        "   \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DygEtRF3rPGU"
      },
      "source": [
        "**Experiments with non-personalized model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASELINE - NO AUGMENTATION\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 8.928,\n",
            "           dias_mae: 6.974, top_n: steps; bed_time; floors; active_calories; wo_calories, second run: False, bootstrap: True\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 10.162,\n",
            "           dias_mae: 8.206, top_n: active_calories; steps; wo_calories; floors; bed_time, second run: True, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "K-ROLL AUGMENTATION\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 8.968,\n",
            "           dias_mae: 7.042, top_n: steps; active_calories; bed_time; floors; wo_calories, second run: False, bootstrap: True\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 10.383,\n",
            "           dias_mae: 8.317, top_n: steps; floors; wo_calories; active_calories; bed_time, second run: True, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "KNN INTRA AUGMENTATION\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 8.914,\n",
            "           dias_mae: 7.033, top_n: awake_count; active_calories; steps; wo_calories; bed_time, second run: False, bootstrap: True\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 10.132,\n",
            "           dias_mae: 7.888, top_n: steps; wo_calories; bed_time; active_calories; awake_count, second run: True, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "KNN INTER AUGMENTATION\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 8.824,\n",
            "           dias_mae: 7.024, top_n: steps; bed_time; sleep_minutes; active_calories; awake_count, second run: False, bootstrap: True\n",
            "dataset size: (2102, 13), model: xgb, ntrees: 60, sys_mae: 9.965,\n",
            "           dias_mae: 8.212, top_n: awake_count; sleep_minutes; bed_time; active_calories; steps, second run: True, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from utils import historical_BP\n",
        "\n",
        "############################################# PARAMETERS #############################################\n",
        "N = 5                               # Number of most important features to display\n",
        "model = 'xgb'                       # rf or xgb (Random Forest or XGBoost)\n",
        "ntrees = 60                         # Number of trees in the forest\n",
        "double_run = False                  # Whether to use a second run with top N features or not\n",
        "bootstrap = True                    # Whether to use bootstrap samples\n",
        "bootstrap_size = 0.8                # Portion of the dataset to sample for bootstrap\n",
        "key = ['healthCode', 'date']        # Columns to use as key\n",
        "target = ['systolic', 'diastolic']  # Columns to predict\n",
        "log_path = 'exp_log.csv'            # Path of file to log experiment results\n",
        "historical = True                  # Whether to use historical BP or not\n",
        "\n",
        "\n",
        "############################################# EXPERIMENTS #############################################\n",
        "# Predicting systolic BP using baseline with non NaN values\n",
        "print('BASELINE - NO AUGMENTATION')\n",
        "dataset = baseline\n",
        "aug = 'None'\n",
        "experiment(dataset, model, ntrees, N, key, target, log_path=log_path, \n",
        "           bootstrap=bootstrap, bootstrap_size=bootstrap_size, aug=aug, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "# Predicting systolic BP using k-roll augmentation with non NaN values\n",
        "print('K-ROLL AUGMENTATION')\n",
        "dataset = augmented_k\n",
        "aug = 'K-roll'\n",
        "experiment(dataset, model, ntrees, N, key, target, log_path=log_path, \n",
        "           bootstrap=bootstrap, bootstrap_size=bootstrap_size, aug=aug, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# Predicting systolic BP using knn intra augmentation with non NaN values\n",
        "print('KNN INTRA AUGMENTATION')\n",
        "dataset = augmented_intra\n",
        "aug = 'KNN-intra'\n",
        "experiment(dataset, model, ntrees, N, key, target, log_path=log_path, \n",
        "           bootstrap=bootstrap, bootstrap_size=bootstrap_size, aug=aug, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# Predicting systolic BP using knn inter augmentation with non NaN values\n",
        "print('KNN INTER AUGMENTATION')\n",
        "dataset = augmented_inter\n",
        "aug = 'KNN-inter'\n",
        "experiment(dataset, model, ntrees, N, key, target, log_path=log_path, \n",
        "           bootstrap=bootstrap, bootstrap_size=bootstrap_size, aug=aug, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Experiments with personalized model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASELINE - NO AUGMENTATION\n",
            "dataset size: (193, 11), model: xgb, ntrees: 60, sys_mae: 21.0,\n",
            "           dias_mae: 5.0, top_n: N/A, second run: False, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "K-ROLL AUGMENTATION\n",
            "dataset size: (245, 11), model: xgb, ntrees: 60, sys_mae: 21.0,\n",
            "           dias_mae: 5.0, top_n: N/A, second run: False, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "KNN INTRA AUGMENTATION\n",
            "dataset size: (193, 11), model: xgb, ntrees: 60, sys_mae: 7.702,\n",
            "           dias_mae: 7.619, top_n: N/A, second run: False, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n",
            "KNN INTER AUGMENTATION\n",
            "dataset size: (193, 11), model: xgb, ntrees: 60, sys_mae: 9.601,\n",
            "           dias_mae: 6.777, top_n: N/A, second run: False, bootstrap: True\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "############################################# PARAMETERS #############################################\n",
        "N = 5                               # Number of most important features to display\n",
        "ntrees = 60                         # Number of trees in the forest\n",
        "bootstrap = True                   # Whether to use bootstrap samples\n",
        "bootstrap_size = 0.8                # Portion of the dataset to sample for bootstrap\n",
        "key = ['healthCode', 'date']        # Columns to use as key\n",
        "target = ['systolic', 'diastolic']  # Columns to predict\n",
        "log_path = 'exp_log.csv'            # Path of file to log experiment results\n",
        "save_path = 'user_data'          # Path to save personalized models\n",
        "historical = False                  # Whether to use historical BP or not\n",
        "\n",
        "\n",
        "############################################# EXPERIMENTS #############################################\n",
        "# Predicting BP using baseline with non NaN values\n",
        "print('BASELINE - NO AUGMENTATION')\n",
        "aug = 'None'\n",
        "dataset = baseline\n",
        "personalized_experiment(dataset, 'xgb', ntrees, N, key, target, log_path=log_path, bootstrap=bootstrap, \n",
        "                        bootstrap_size=bootstrap_size, aug=aug, save_path=save_path, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "# Predicting BP using k-roll augmentation with non NaN values\n",
        "print('K-ROLL AUGMENTATION')\n",
        "dataset = augmented_k\n",
        "aug = 'K-roll'\n",
        "personalized_experiment(dataset, 'xgb', ntrees, N, key, target, log_path=log_path, bootstrap=bootstrap, \n",
        "                        bootstrap_size=bootstrap_size, aug=aug, save_path=save_path, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# Predicting BP using knn intra augmentation with non NaN values\n",
        "print('KNN INTRA AUGMENTATION')\n",
        "dataset = augmented_intra\n",
        "aug = 'KNN-intra'\n",
        "personalized_experiment(dataset, 'xgb', ntrees, N, key, target, log_path=log_path, bootstrap=bootstrap, \n",
        "                        bootstrap_size=bootstrap_size, aug=aug, save_path=save_path, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# Predicting BP using knn inter augmentation with non NaN values\n",
        "print('KNN INTER AUGMENTATION')\n",
        "dataset = augmented_inter\n",
        "aug = 'KNN-inter'\n",
        "personalized_experiment(dataset, 'xgb', ntrees, N, key, target, log_path=log_path, bootstrap=bootstrap, \n",
        "                        bootstrap_size=bootstrap_size, aug=aug, save_path=save_path, historical=historical)\n",
        "print('--------------------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import load_user_model, load_json_as_dict\n",
        "import os\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "\n",
        "def get_recommendations(entry, model_path, f_imp_path, dataset_path, key, target, n=5, var_adjust=False):\n",
        "    model_files = os.listdir(model_path)\n",
        "    ids = [f.split('_')[0] for f in model_files]\n",
        "    id = ids[entry]     # Extract the id of the user to make recommendations for\n",
        "\n",
        "    # Load the models for the user\n",
        "    model_sys = load_user_model(f'{model_path}/{id}_systolic.json')\n",
        "    model_dia = load_user_model(f'{model_path}/{id}_diastolic.json')\n",
        "\n",
        "    # Get the feature importances for the user\n",
        "    file_path = f'{f_imp_path}/{id}.json'\n",
        "    feature_importances = load_json_as_dict(file_path)\n",
        "    top_n = dict(islice(feature_importances.items(), n))\n",
        "\n",
        "\n",
        "    # Get predictor values for one of the user's test cases\n",
        "    test_dataset = pd.read_csv(f'{dataset_path}/test.csv')\n",
        "    test_entry = test_dataset[test_dataset['healthCode'] == id].iloc[[0]]\n",
        "\n",
        "    # Generate predictions for boths types of bp for the test entry\n",
        "    expected_sys = 120.0\n",
        "    expected_dia = 80.0\n",
        "    x = test_entry.drop(key + target, axis=1)\n",
        "    # print datatypes of all of x columns\n",
        "    x = x.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    sys_prediction = model_sys.predict(x)\n",
        "    print(f'Predicted value: {sys_prediction}')\n",
        "    sys_to_correct = sys_prediction - expected_sys\n",
        "    dia_prediction = model_dia.predict(x)\n",
        "    print(f'Predicted value: {dia_prediction}')\n",
        "    dia_to_correct = dia_prediction - expected_dia\n",
        "\n",
        "    if sys_to_correct < 0:\n",
        "        sys_to_correct = 0\n",
        "    if dia_to_correct < 0:\n",
        "        dia_to_correct = 0\n",
        "\n",
        "    total = expected_sys + expected_dia\n",
        "    sys_w = expected_dia / total\n",
        "    dia_w = expected_sys / total\n",
        "\n",
        "    # Get weighter combination of systolic and diastolic to correct\n",
        "    bp_to_correct = (sys_w * sys_to_correct + dia_w * dia_to_correct) / 2\n",
        "    print('Weighted correction:', bp_to_correct)\n",
        "\n",
        "    if bp_to_correct <= 0:\n",
        "        print('No correction needed')\n",
        "        return\n",
        "    \n",
        "    if var_adjust:\n",
        "        # summ all the top n feature importances values\n",
        "        var_explained = sum(top_n.values())\n",
        "        pred_adjustment = bp_to_correct / var_explained\n",
        "        # Adjust the top n feature importances\n",
        "        for key in top_n.keys():\n",
        "            top_n[key] *= pred_adjustment\n",
        "\n",
        "    # Multiply each top n prediction value by its corresponding feature importance\n",
        "    recs = {}\n",
        "    for key in top_n.keys():\n",
        "        recs[key] = x[key] * top_n[key]\n",
        "        print(f'key: {key}  -   value: {x[key].item()}   -  f_score: {top_n[key]}  -  rec: {recs[key].item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted value: [115.56446]\n",
            "Predicted value: [80.0675]\n",
            "Weighted correction: [0.02024918]\n",
            "key: steps  -   value: 5102.0   -  f_score: 0.13045477867126465  -  rec: 665.5802807807922\n",
            "key: bed_time  -   value: 955.0231794263708   -  f_score: 0.12088577449321747  -  rec: 115.44871670393182\n",
            "key: sleep_minutes  -   value: 321.08017579491   -  f_score: 0.11633481085300446  -  rec: 37.35280151975028\n",
            "key: active_calories  -   value: 0.0   -  f_score: 0.11332559585571289  -  rec: 0.0\n",
            "key: wo_calories  -   value: 243.72729600887163   -  f_score: 0.11147531121969223  -  rec: 27.169576175323016\n"
          ]
        }
      ],
      "source": [
        "# Get recommendations for one of the users in the testing set\n",
        "entry = 33                 # Choose from 0 to 58\n",
        "n = 5                     # Number of most important features to display\n",
        "model_path = 'user_data/model_states'\n",
        "f_imp_path = 'user_data/feature_importances'\n",
        "dataset_path = 'data/train_test'\n",
        "key = ['healthCode', 'date']        # Columns to use as key\n",
        "target = ['systolic', 'diastolic']  # Columns to predict\n",
        "var_adjust = False\n",
        "\n",
        "get_recommendations(entry, model_path, f_imp_path, dataset_path, key, target, n=n, var_adjust=var_adjust)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
